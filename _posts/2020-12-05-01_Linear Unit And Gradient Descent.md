---
layout: post
title: 线性单元和梯度下降
date: 2020-12-05
author: Zhiliang 
tags: [Machine Learning]
toc: true
mathjax: true
---

感知器有一个问题，当面对的数据集不是**线性可分**的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个**可导**的**线性函数**来替代感知器的**阶跃函数**，这种感知器就叫做**线性单元**。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。

<!-- more -->

# 线性单元是什么

感知器有一个问题，当面对的数据集不是**线性可分**的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个**可导**的**线性函数**来替代感知器的**阶跃函数**，这种感知器就叫做**线性单元**。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。

为了简单起见，我们可以设置线性单元的激活函数$f$为

$$
f\left( x \right) =x \tag{1.1}
$$

这样的线性单元如下图所示

![](https://gitee.com/zhiliangj/Typora_Img/raw/master/2256672-f57602e423d739ee.png)

对比此前我们讲过的感知器

![](https://gitee.com/zhiliangj/Typora_Img/raw/master/2256672-801d65e79bfc3162.png)

这样替换了激活函数$f$之后，**线性单元**将返回一个**实数值**而不是**0,1分类**。因此线性单元用来解决**回归**问题而不是**分类**问题。

## 线性单元的模型

当我们说**模型**时，我们实际上在谈论根据输入$x$预测输出$y$的算法。比如：$x$可以是一个人的工作年限，$y$可以用某种算法来根据一个人的工作年限来预测他的收入。比如：

$$
y=h(x)=w*x+b\tag{1.2}
$$

函数$h(x)$叫做**假设**，而$w$、$b$是它的参数。我们假设参数$w=1000$，参数$b=500$，如果一个人的工作年限是5年的话，我们的模型会预测他的月薪为：

$$
y=h\left( x \right) =1000*5+500=5500\left( \text{元} \right)\tag{1.3}
$$

这个模型明显不太靠谱了。的确是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为**特征**。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他。

$$
\text{x}=\left( 5,IT,\text{百度,}T6 \right)\tag{1.4}
$$

既然输入$x$变成了一个具备四个特征的向量，相对应的，仅仅一个参数$w$就是不够用了，我们应该使用4个参数$w_1,w_2,w_3,w_4$，每个特征对应一个。这样，我们的模型就变成

$$
y=h\left( x \right) =w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b\tag{1.5}
$$

其中，$x_1$对应工作年限，$x_2$对应行业，$x_3$对应公司，$x_4$对应职级。

为了书写和计算方便，我们可以令$w_0$等于$b$，同时令$w_0$对应于特征$x_0$。由于$x_0$其实并不存在，我们可以令它的值为1。也就是说：

$$
b=w_0*x_0  其中x_0=1\tag{1.6}
$$

这样上面的例子就可以写成：

$$
\begin{align}
y=h\left( x \right) &=w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b\\
&=w_0*x_0+w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4
\end{align}\tag{1.7}
$$

我们还可以把上式写成向量的形式

$$
y=h\left( x \right) =\text{w}^T\text{x}\tag{1.7}
$$

长成这种样子模型就叫做**线性模型**，因为输出$y$就是输入特征$x_1,x_2,x_3,\cdots$的线性组合

## 监督学习和无监督学习

接下来，我们需要关心的是这个模型如何训练，也就是参数$\text{w}$取什么值最合适。

机器学习有一类学习方法叫做**监督学习**，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征$\text{x}$，也包括对应的输出$y$($y$也叫做**标记，label**)。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业...)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征$\text{x}$)，也看到对应问题的答案(标记$y$)。当模型看到足够多的样本之后，他就能总结出其中的一些规律。然后，就可以预测那些他没看过的输入所对应的答案了。

另外一类学习方法叫做**无监督学习**，这种方法的训练样本中只有$x$而没有$y$。模型可以总结出特征$x$的一些规律，但是无法知道其对应的答案$y$。

很多时候，既有$x$又有$y$的训练样本是很少的，大部分样本都只有$x$。

比如在语音到文本(STT)的识别任务中，$x$是语音，$y$是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并标注**上对**应文字则是非常费力的事情。这种情况下，为了弥补带标注样本的不足，我们可以用**无监督学习方法**先做一些**聚类**，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。

## 线性单元的目标函数

现在，让我们只考虑**监督学习**。

在监督学习下，对于一个样本，我们知道它的特征$x$，以及标记$y$。同时，我们还可以根据模型$h(x)$计算得到输出$\overline{y}$。注意这里我们用$y$表示训练样本里面的**标记**，也就是**实际值**；用带上划线的$\overline{y}$表示计算的出来的预测值。我们当然希望模型计算出来的$\overline{y}$和$y$越接近越好。

数学上有很多方法来表示的$\overline{y}$和$y$的接近程度，比如我们可以用$\overline{y}$和$y$的差的平方的$\frac{1}{2}$来表示他们的接近程度

$$
e=\frac{1}{2}\left( y-\overline{y} \right) ^2\tag{1.8}
$$

我们把$e$叫做单个样本的误差。至于为什么前面要乘$\frac{1}{2}$，是为了后面计算方便。

训练数据中会有很多样本，比如个$N$，我们可以用训练数据中**所有样本**的误差的**和**，来表示模型的误差，也就是

$$
E=e^{\left( 1 \right)}+e^{\left( 2 \right)}+e^{\left( 3 \right)}+\cdots +e^{\left( n \right)}\tag{1.9}
$$

上式中的$e^{\left( 1 \right)}$表示第一个样本的误差，$e^{\left( 2 \right)}$表示第二个样本的误差。

$$
\begin{align}
	E&=e^{\left( 1 \right)}+e^{\left( 2 \right)}+e^{\left( 3 \right)}+\cdots +e^{\left( n \right)}\\
	&=\sum_{i=1}^n{e^{\left( i \right)}}\\
	&=\frac{1}{2}\sum_{i=1}^n{\left( y^{\left( i \right)}-\overline{y}^{\left( i \right)} \right) ^2}\\
\end{align} \tag{1.10}
$$

其中

$$
\begin{align}
	\overline{y}^{\left( i \right)}&=h\left( \text{x}^{\left( i \right)} \right)\\
	&=\text{w}^T\text{x}^{\left( \text{i} \right)}\\
\end{align}\tag{1.11}
$$

式1.11中，$x^{(i)}$表示第$i$个训练样本的**特征**，$y^{(i)}$表示第$i$个样本的标记。我们也可以用**元组**$(x^{(i)},y^{(i)})$表示第$i$个**训练样本**。$\overline{y}^{(i)}$则是模型对第$i$个样本的**预测值**。

我们当然希望对于一个训练数据集来说，误差最小越好，也就是(式1.10)的值越小越好。对于特定的训练数据集来说，$(x^{(i)},y^{(i)})$的值都是已知的，所以式1.10其实是参数$\text{w}$的函数。

$$
\begin{matrix}
	E\left( \text{w} \right) &=\frac{1}{2}\sum_{i=1}^n{\left( y^{\left( i \right)}-\overline{y}^{\left( i \right)} \right) ^2}\\
	&=\frac{1}{2}\sum_{i=1}^n{\left( \text{y}^{\left( \text{i} \right)}-\text{w}^{\text{T}}\text{x}^{\left( \text{i} \right)} \right) ^2}\\
\end{matrix}\tag{1.12}
$$

由此可见，模型的训练，实际上就是求取到合适的$\text{w}$，使式1.10取得最小值。这在数学上称作**优化问题**，而$E(\text{w})$就是我们优化的目标，称之为**目标函数**。

## 梯度下降优化算法

大学时我们学过怎样求函数的极值。函数$y=f(x)$的极值点，就是它的倒数$f'(x)=0$的那个点。因此我们可以通过解方程$f'(x)=0$，求得函数的极值点$(x_0,y_0)$。

不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点**『试』**出来。如下图所示：

![](https://gitee.com/zhiliangj/Typora_Img/raw/master/2256672-46acc2c2d52fc366.png)

首先，我们随便选择一个点开始，比如上图的$x_0$点。每次迭代修改$x$的值为$x_1,x_2,x_3,...$，经过数次迭代后最终达到函数最小值点。

你可能要问了，为啥每次修改$x$的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数$y=f(x)$的**梯度**的**相反方向**来修改。什么是**梯度**呢？翻开大学高数课的课本，我们会发现**梯度**是一个向量，它指向**函数值上升最快**的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。

按照上面的讨论，我们就可以写出梯度下降算法的公式

$$
\mathrm{x}_{new}=\mathrm{x}_{old}-\eta\nabla{f(x)}\tag{1.13}
$$

其中，$\nabla$是**梯度算子**，$\nabla f(x)$就是指$f(x)$的梯度。$\eta$是步长，也称作**学习速率**。

对于上一节列出的目标函数式1.10

$$
E(\mathrm{w})=\frac{1}{2}\sum_{i=1}^{n}(\mathrm{y^{(i)}-\bar{y}^{(i)}})^2 \tag{1.14}
$$

梯度下降算法可以写成

$$
\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\nabla{E(\mathrm{w})} \tag{1.15}
$$

聪明的你应该能想到，如果要求目标函数的**最大值**，那么我们就应该用**梯度上升**算法，它的参数修改规则是

$$
\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\nabla{E(\mathrm{w})} \tag{1.16}
$$

下面，请先做几次深呼吸，让你的大脑补充足够的新鲜的氧气，我们要来求取$\nabla E(\text{w})$，然后带入上式，就能得到线性单元的参数修改规则。

关于$\nabla E(\text{w})$的推导过程，其实就是$E(\text{w})$关于$\text{w}$的导数。您既可以选择自己推导一下。在这里，您只需要知道，经过一大串推导，目标函数$\nabla E(w)$的梯度是

> 原文中此处是：
>
> 关于$\nabla E(\text{w})$的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。

$$
\nabla E\left( \text{w} \right) =-\sum_{i=1}^n{\left( y^{\left( i \right)}-\overline{y}^{\left( i \right)} \right) \text{x}^{\left( i \right)}}\tag{1.17}
$$

因此，线性单元的参数修改规则最后是这个样子

$$
\text{w}_{new}=\text{w}_{old}+\eta \sum_{i=1}^n{\left( y^{\left( i \right)}-\overline{y}^{\left( i \right)} \right) \text{x}^{\left( i \right)}}\tag{1.18}
$$

有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。

需要说明的是，如果每个样本有M个特征，则上式中的$\text{x},\text{w}$都是M+1维**向量**(因为我们加上了一个恒为1的虚拟特征${x_0}$，参考前面的内容)，而$y$是**标量**。用高逼格的数学符号表示，就是

$$
\begin{matrix}{l}
	\text{x,w}\in \Re ^{\left( M+1 \right)}\\
	y\in \Re ^1\\
\end{matrix}\tag{1.19}
$$

为了让您看明白说的是啥，我吐血写下下面这个解释(写这种公式可累可累了)。因为$\text{w,x}$是M+1维**列向量**，所以(式3)可以写成

$$
\left[ \begin{array}{c}
	w_0\\
	w_1\\
	w_2\\
	\vdots\\
	w_m\\
\end{array} \right] _{new}=\left[ \begin{array}{c}
	w_0\\
	w_1\\
	w_2\\
	\vdots\\
	w_m\\
\end{array} \right] _{old}+\eta \sum_{i=1}^n{\left( y^{\left( \text{i} \right)}-\overline{y}^{\left( \text{i} \right)} \right)}\left[ \begin{array}{c}
	1\\
	x_{1}^{\left( i \right)}\\
	x_{2}^{\left( i \right)}\\
	\vdots\\
	x_{m}^{\left( i \right)}\\
\end{array} \right]\tag{1.20}
$$

如果您还是没看明白，建议您也吐血再看一下大学时学过的**《线性代数》**吧。

## $\nabla{E}(\mathrm{w})$的推导

这一节你尽可以跳过它，并不太会影响到全文的理解。当然如果你非要弄明白每个细节，那恭喜你骚年，机器学习的未来一定是属于你的。

首先，我们先做一个简单的前戏。我们知道函数的梯度的定义就是它相对于各个变量的**偏导数**，所以我们写下下面的式子

$$
\begin{align}
\nabla{E(\mathrm{w})}&=\frac{\partial}{\partial\mathrm{w}}E(\mathrm{w})\\
&=\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\
\end{align}\tag{1.21}
$$

可接下来怎么办呢？我们知道**和的导数等于导数的和**，所以我们可以先把求和符号里面的导数求出来，然后再把它们加在一起就行了，也就是

$$
\begin{align}
&\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\
=&\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\
\end{align}\tag{1.22}
$$

现在我们可以不管高大上的$\sum$了，先专心把里面的导数求出来。

$$
\begin{align}
&\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\
=&\frac{\partial}{\partial\mathrm{w}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\\
\end{align}\tag{1.23}
$$

我们知道，$y$是与$\text{w}$无关的常数，而$\overline{y}=\text{w}^{T}\text{x}$，下面我们根据链式求导法则来求导(上大学时好像叫复合函数求导法则)

$$
\frac{\partial{E(\mathrm{w})}}{\partial\mathrm{w}}=\frac{\partial{E(\bar{y})}}{\partial\bar{y}}\frac{\partial{\bar{y}}}{\partial\mathrm{w}}\tag{1.23}
$$

我们分别计算上式等号右边的两个偏导数

$$
\begin{align}
\frac{\partial{E(\mathrm{w})}}{\partial\bar{y}}=
&\frac{\partial}{\partial\bar{y}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\\
=&-2y^{(i)}+2\bar{y}^{(i)}\\\\
\frac{\partial{\bar{y}}}{\partial\mathrm{w}}=
&\frac{\partial}{\partial\mathrm{w}}\mathrm{w}^T\mathrm{x}\\
=&\mathrm{x}
\end{align}\tag{1.24}
$$

代入，我们求得里面$\sum$的偏导数是

$$
\begin{align}
&\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\
=&2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}
\end{align}\tag{1.25}
$$

最后代入$\nabla{E}(\mathrm{w})$，求得

$$
\begin{align}
\nabla{E(\mathrm{w})}&=\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\
&=\frac{1}{2}\sum_{i=1}^{n}2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}\\
&=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}
\end{align}\tag{1.26}
$$

至此，大功告成。

## 随机梯度下降算法(Stochastic Gradient Descent, SGD)

如果我们根据(式1.18)来训练模型，那么我们每次更新$\mathrm{w}$的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做**批梯度下降(Batch Gradient Descent)**。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新$\mathrm{w}$的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对$\mathrm{w}$更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新$\mathrm{w}$并不一定按照减少$E$的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别

![](https://gitee.com/zhiliangj/Typora_Img/raw/master/2256672-3152002d503d768e.png)

如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。

最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。
